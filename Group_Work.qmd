---
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: BugAvenger's Group Project
execute:
  echo: false
  freeze: true
format:
  html:
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: "Roboto Flex"
    monofont: InputMonoCondensed
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

## Declaration of Authorship {.unnumbered .unlisted}

We, BugAvenger, pledge our honour that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.

Date: 16/12/2024

Student Numbers: 5

## Brief Group Reflection

| What Went Well | What Was Challenging |
| -------------- | -------------------- |
| A              | B                    |
| C              | D                    |

## Priorities for Feedback

Are there any areas on which you would appreciate more detailed feedback if we're able to offer it?

{{< pagebreak >}}



```{python}
# Reproducible Code
# Install and import packages

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import geopandas as gpd
from nltk.tokenize import word_tokenize 
from nltk.corpus import stopwords 
from gensim import corpora,models,similarities 
from geopy.distance import geodesic 
from shapely import wkt 
import os
```

```{python}
# Read Inside Airbnb data

cols = ['id', 'name', 'description', 'neighborhood_overview', 'host_id','neighbourhood_cleansed', 
        'latitude','longitude', 'room_type', 'price', 'minimum_nights_avg_ntm', 
        'maximum_nights_avg_ntm', 'availability_30','number_of_reviews', 'review_scores_rating', 
        'review_scores_accuracy','calculated_host_listings_count', 'reviews_per_month']

# Read the listing.csv.gz compressed file through the URL
df = pd.read_csv('https://github.com/Cihshee/CASA0013_BugAvenger/raw/refs/heads/main/data/listings.csv.gz', encoding='latin1', low_memory=False, usecols=cols)

# Print the dimensions of the loaded DataFrame
print(f"Data frame is {df.shape[0]:,} x {df.shape[1]}")
```

```{python}
# Calculate the total number of missing values for each column
# Sort the results in descending order to see columns with the most missing values first
df.isnull().sum(axis=0).sort_values(ascending=False)
```

```{python}
# Drop rows where the 'id' column has missing (NaN) values
# 'inplace=True' applies the change directly to the DataFrame
df.drop(df[df.id.isna()].index.values, axis=0, inplace=True)
```

```{python}
# Next, we check for rows that still have lots of NaN.
# From following output, we find several rows have 6 NaNs. By checking, we found that these rows also lost a lot of important information that may be used in subsequent analysis (such as "latitude", "longitude", "host_id").
# Therefore, these rows which lost much important information are also needed to be deleted.
df.isnull().sum(axis=1).sort_values(ascending=False)
```

```{python}
# Filter rows where the 'longitude' and 'latitude' column can be converted to a numeric value
# 'errors="coerce"' replaces invalid parsing with NaN, and 'notna()' keeps only valid rows
df = df[pd.to_numeric(df['longitude'], errors='coerce').notna()]
df = df[pd.to_numeric(df['latitude'], errors='coerce').notna()]

df.drop(df[df.longitude.isna()].index.values, axis=0, inplace=True)
df.drop(df[df.latitude.isna()].index.values, axis=0, inplace=True)

df.drop(df[df.neighborhood_overview.isna() & df.description.isna()].index.values, axis=0, inplace=True)

# Print the number of rows remaining in the DataFrame after all filtering
# print(df.shape[0])
```        

```{python}
# Remove "$" symbol and try to convert to float number
df['price'] = df['price'].str.replace('$','').str.strip()
df['price'] = pd.to_numeric(df['price'], errors='coerce')

df['maximum_nights_avg_ntm'] = pd.to_numeric(df['maximum_nights_avg_ntm'], errors='coerce') 
df['minimum_nights_avg_ntm'] = pd.to_numeric(df['minimum_nights_avg_ntm'], errors='coerce')
df['reviews_per_month'] = pd.to_numeric(df['reviews_per_month'], errors='coerce') 
df['calculated_host_listings_count'] = pd.to_numeric(df['calculated_host_listings_count'], errors='coerce') 
df['availability_30'] = pd.to_numeric(df['availability_30'], errors='coerce') 
df['number_of_reviews'] = pd.to_numeric(df['number_of_reviews'], errors='coerce') 
df['review_scores_rating'] = pd.to_numeric(df['review_scores_rating'], errors='coerce') 
df['review_scores_accuracy'] = pd.to_numeric(df['review_scores_accuracy'], errors='coerce') 
df['review_scores_accuracy'] = pd.to_numeric(df['review_scores_accuracy'], errors='coerce') 

# Filter rows by room type
# Choose specify columns to include
houseList = df.loc[df["room_type"].isin(["Entire home/apt","Private room","Shared room"]),["id","host_id","room_type","description","neighborhood_overview","longitude","latitude","calculated_host_listings_count","neighbourhood_cleansed"]]

print(houseList.shape[0])

```


```{python}
# Group the 'houseList' DataFrame by the 'neighbourhood_cleansed' column
# For each neighborhood group, count the occurrences of each 'room_type'
houseList.groupby('neighbourhood_cleansed')['room_type'].value_counts()
```


```{python}
# Check for duplicate values in the 'id' column
duplicate_count = df['id'].duplicated(keep=False).sum()
print(f"the count of duplicate id: {duplicate_count}")
```

```{python}
# Define sgensimSimilarities() and gensimilarities()

# nltk.download('stopwords')
# nltk.download('punkt')
# nltk.download('punkt_tab')

import requests

response = requests.get('https://raw.githubusercontent.com/Cihshee/CASA0013_BugAvenger/main/data/London_Spatial_Stopwords_list_1126.CSV')
response.raise_for_status()  # Check if the request was successful
stopWords_london = set(response.text.split())  # Split the text content into a set of words

# Get a collection of common English words("the", "is", "and", etc.), which will be excluded from the similarity calculation
stopWords = set(stopwords.words('english')) 
stopWords = stopWords | stopWords_london 

# Check the stopwords_list
print(stopWords_london)
print(stopWords)
```


```{python}
def gensimSimilarities(test, textList):
    """
    if textList only contain one element, we take turns to use the two text as test
    """
    if len(textList)==1: 
        sim = [max(gensimilarities(test, textList))] 
        textList.append(test) 
        test = textList.pop(0)
        sim.append(max(gensimilarities(test, textList))) 
        # use the small one as the similarity
        sim = min(sim) 
    else:
        sim = max(gensimilarities(test, textList))
    return sim
```


```{python}
def gensimilarities(test,textList):
    """
    use tf-idf to construst the importance of the words, and use the gensim to calculate the similarity
    """
    allWordsList = []
    # when there are only one element, the function will always return 0
    if len(textList)==1:
        textList.append("0") 
    for text in textList:
        # separate the words and exclude the words in stopwords
        wordsList = [word for word in word_tokenize(text) if word not in stopWords]
        allWordsList.append(wordsList)
    dictionary = corpora.Dictionary(allWordsList)
    corpus = [dictionary.doc2bow(wordsList1) for wordsList1 in allWordsList]    
    # Convert the test text into BOW format and filter for disused words
    testWords = dictionary.doc2bow([word for word in word_tokenize(test) if word not in stopWords])
    # Build TF-IDF models and similarity indexes
    tfidf = models.TfidfModel(corpus)
    index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=len(dictionary.keys()))
    sim = index[tfidf[testWords]]
    return sim
```

```{python}
# Define checkDescription() that determines if a room description or neighborhood information is highly similar to the description of a set of landlord listings
# If the similarity exceeds a threshold (0.5), the room is categorized as ghost hotel

def checkDescription(checkroom,ghostHotel):
    """
    Define checkDescription() that determines if a room description or neighborhood information is highly similar to the description of a set of landlord listings
    
    If the similarity exceeds a threshold (0.5), the room is categorized as ghost hotel
    """
    checkDf = houseList.loc[houseList["id"]==checkroom,["id", "neighborhood_overview","description"]]
    samehostDf = houseList[houseList["id"].isin(ghostHotel)]
    simlarity1 = 0
    simlarity2 = 0
    overviewDf = samehostDf["neighborhood_overview"].dropna()
    checkOverview = checkDf["neighborhood_overview"].dropna()

    if overviewDf.shape[0]>0 and checkOverview.shape[0] > 0:
        simlarity1 = gensimSimilarities(checkOverview.iloc[0],list(overviewDf)) 

    if samehostDf.shape[0] != overviewDf.shape[0]: 
        descripDf = samehostDf["description"].dropna()
        checkDesc = checkDf["description"].dropna()
        if descripDf.shape[0]>0 and checkDesc.shape[0] > 0: 
            simlarity2 = gensimSimilarities(checkDesc.iloc[0], list(descripDf)) 
    simlarity = max(simlarity1, simlarity2)

    # If the similarity exceeds a threshold (0.5), the room is categorized as ghost hotel
    if simlarity > 0.5:
        return True
    else:
        return False
```

```{python}
# Define mergeSubList()

def mergeSubList(initiallist):
    """
    If there are same id in different sublist, merge the sublist
    """
    length = len(initiallist)
    for init1 in range(length):
        for init2 in range(init1 + 1, length):
            if initiallist[init1] == [0]:
                break
            if initiallist[init2] == [0]:
                continue

            # to see whether the two sublists have same elements
            x = list(set(initiallist[init1]+initiallist[init2]))
            y = len(initiallist[init1]) + len(initiallist[init2])           
            
            # If the merged length is less, there is a duplicate
            if len(x) < y:
                initiallist[init2] = [0]
                initiallist[init1] = x
    return initiallist
```

```{python}
# The main part to find the ghost hotels

# Creat a list called "ghostlist" which contains all the 'id' for ghost rooms, and each sublist of ghostlist is a "ghost hotel".

# - Initialize data and temporary rows
# - Iterate through listings:
#    - Filter a subset of listings by host
#    - Skip if the host has <= 1 listing
#    - Group by geographic location
#    - Group by text description
#    - Merge groups and retain valid groups
# - Update results (ghostList) and unprocessed listing indices
# - Remove temporary rows and clean the data

# A listing must meet the following criteria to be classified as a "ghost hotel":

# - Geographical condition: Distance is less than 200 meters.
# - Description condition: Text descriptions are similar.

indexlist = list(houseList.index)
ghostList = []

houseList.loc["ini",["id","host_id","description","neighborhood_overview","longitude","latitude","calculated_host_listings_count"]] = [0,0,"0","0",0,0,0]
while len(indexlist) > 0 :
    # if one have more than one private or shared rooms on Airbnb, it has the suspicion to be ghost hotels
    if houseList.loc[indexlist[0],"calculated_host_listings_count"] > 1:
        # find the house which have the same host_id and put them together
        ghostdf = houseList[houseList["host_id"] == houseList.loc[indexlist[0],"host_id"]]
        ghostHost = list(ghostdf['id'])
        # it is a list used to store the ghost hotels list
        initiallist = [[0]]
        for i in ghostdf.index:
            # if there are no rooms within the distance it will still be False
            isghost = False
            for init in initiallist:
                for ini in init:
                    # the longitude and latitude of the rooms in list
                    iniLogLat = (houseList.loc[houseList["id"]==ini,"longitude"].iloc[0],houseList.loc[houseList["id"]==ini,"latitude"].iloc[0])
                    iLonLat = (ghostdf.loc[i,"longitude"],ghostdf.loc[i,"latitude"])
                    # use the longitude and latitude to calculate the distance
                    distance = geodesic(iniLogLat, iLonLat).m
                    # if the distance is less than 200m, we consider it as ghost hotel
                    #(distance of 50 to 500 meters is manually tried and find the optimal one)
                    if distance < 200:
                        init.append(ghostdf.loc[i,"id"])
                        isghost = True
                        break
            if not isghost:
                initiallist.append([ghostdf.loc[i,"id"]])
        
        # use the description to calculate the distance, in case of inaccurate positioning
        initiallist1 = [[0]]
        for checkroom in ghostHost:
            isghost = False
            for ghostHotel in initiallist1:
                if checkDescription(checkroom,ghostHotel): 
                    ghostHotel.append(checkroom)
                    isghost = True
                    break
            if not isghost:
                initiallist1.append([checkroom])
        initiallist.extend(initiallist1)
        initiallist = mergeSubList(initiallist)
        # the list with more than one id is a ghost hotel
        initiallist = [init for init in initiallist if len(init)>1]
        ghostList.extend(initiallist)
        # delete the rows that we have already checked
        indexlist = [index for index in indexlist if index not in ghostdf.index]

    else:
        # if the host don't have more than one private or shared rooms, it is not ghost hotels
        del indexlist[0]
        
houseList.drop("ini",inplace=True)
```

```{python}
ghostHotelDf = pd.DataFrame(columns = df.columns) 

for rooms in ghostList:
    # Using id, find the corresponding complete information in the original dataframe df and summarize it in the new dataframe ghostHotelDf
    ghostHotelDf = ghostHotelDf._append(df.loc[df["id"].isin(rooms)])

print(ghostHotelDf.shape[0]) # 49848

```

```{python}
ghostHotelDf.head()

room_type_counts = ghostHotelDf['room_type'].value_counts()
print(room_type_counts)

# Entire home/apt    32952
# Private room       16709
# Shared room          187
```

```{python}
# Processing ghostHotelDf
# We checked the resulting ghostHotelDf and found that there are "id" duplicates in it, so we need to remove the duplicate "id" row.

value_counts_id_0 = ghostHotelDf['id'].value_counts()
print(value_counts_id_0[:10])

ghostHotelDf_unique = ghostHotelDf[~ghostHotelDf['id'].duplicated(keep='first')]

value_counts_id_1 = ghostHotelDf_unique['id'].value_counts()
print(value_counts_id_1[:10])

print(f"Data frame is {ghostHotelDf_unique.shape[0]:,} x {ghostHotelDf_unique.shape[1]}")

# ghostHotelDf_unique: removed duplicates
```

```{python}
# # It was found that some owners have multiple listings (possibly be agents), and based on the visualization results, we need to remove the data with a number of listings > 7

# # Check host_id with multiple listings
# value_counts_host_id_0 = ghostHotelDf_unique['host_id'].value_counts()
# print(value_counts_host_id_0[:10])

# # Check how many listings each landlord owns Number of appearances
# frequency_counts = value_counts_host_id_0.value_counts()
# sorted_frequency_counts = frequency_counts.sort_index()
# #print(sorted_frequency_counts[:50])
# print(sorted_frequency_counts)

# # plot figure
# plt.figure(figsize=(10, 6))
# sns.histplot(value_counts_host_id_0, bins=15, kde=True)
# plt.title("Host ID Occurrence Frequency Distribution")
# plt.xlabel("Number of Occurrences")
# plt.ylabel("Frequency")
# plt.xlim(0, 20)
# plt.show()

# plt.figure(figsize=(10, 6))
# sns.boxplot(x=value_counts_host_id_0)
# plt.title("Boxplot of Host ID Occurrences")
# plt.xlabel("Number of Occurrences")
# plt.xlim(0, 20)
# plt.show()
```

```{python}
# # The line chart shows a sharp decline in frequency beyond 7, indicating a significant drop in the number of hosts with more listings.
# # Using 7 as the threshold balances capturing a manageable group of active hosts while excluding extreme cases for focused analysis.

# hosts_to_remove = ghostHotelDf_unique.loc[ghostHotelDf_unique['calculated_host_listings_count'] > 7, 'host_id'].unique()

# ghostHotelDf_filtered = ghostHotelDf_unique[~ghostHotelDf_unique['host_id'].isin(hosts_to_remove)]

# print(f"Original DataFrame Shape: {ghostHotelDf_unique.shape}")
# print(f"Filtered DataFrame Shape: {ghostHotelDf_filtered.shape}")

# print("Host IDs removed:", hosts_to_remove)
```

```{python}
# Visualization
# Composition of airbnb listings with respect yo ghost hotel

number1Df = pd.DataFrame(df["neighbourhood_cleansed"].value_counts()) # 按区域统计房源数
number2Df = pd.DataFrame(houseList["neighbourhood_cleansed"].value_counts()) # 按区域统计私人房源和共享房源数量
number3Df = pd.DataFrame(ghostHotelDf_filtered["neighbourhood_cleansed"].value_counts()) # 按区域统计“ghost hotel”房源数
numberDf = pd.merge(number1Df, number2Df, how='outer',left_index=True,right_index=True)
numberDf = pd.merge(numberDf, number3Df, how='outer',left_index=True,right_index=True) # 合并三个统计结果
numberDf.columns = ["total listings","private/shared rooms","ghost hotels"] # 命名列
numberDf = numberDf.fillna(0) # 将 NaN 值填补为 0

numberDf.head()
```

```{python}
def stackFigure():
    sns.set_style("white")
    sns.set_context({"figure.figsize": (18, 10)})
    sns.barplot(x = numberDf.index, y = numberDf["total listings"], color = "yellow")
    # sns.barplot(x = numberDf.index, y = numberDf["private/shared rooms"], color = "blue")
    ghost_plot = sns.barplot(x = numberDf.index, y = numberDf["ghost hotels"], color = "red")

    raw_data_legend = plt.Rectangle((0,0),1,1,fc="yellow", edgecolor = 'none')
    # private_room_legend = plt.Rectangle((0,0),2,1,fc='blue',  edgecolor = 'none')
    ghost_hotel_legend = plt.Rectangle((0,0),1,1,fc='red',  edgecolor = 'none')
    font = {'weight' : 'normal','size': 20}
    # l = plt.legend([raw_data_legend, private_room_legend, ghost_hotel_legend], ["total listings", "private/shared rooms","ghost hotel rooms"], ncol = 1, prop={'size':16},loc='upper left')
    l = plt.legend([raw_data_legend, ghost_hotel_legend], ["total listings","ghost hotel rooms"], ncol = 1, prop={'size':16},loc='upper left')
    ghost_plot.set_ylabel("The Amount of Rooms",font)
    ghost_plot.set_title("Figure 1: The Composition of Listings on Airbnb of 33 Boroughs in London",fontsize=25)
    plt.yticks(fontsize= 10)
    plt.xticks(rotation=-90, fontsize=14)
    plt.show()
```



【此处需要插入可视化部分的所有代码，格式需要和上面一样（不能添加markdown文字块，必须全部是python代码块；可以把所有文字化为代码块里的注释）】









# Response to Questions

## 1. Who collected the InsideAirbnb data?

::: {.duedate}
( 2 points; Answer due Week 7 )
:::

@Insideairbnb: https://insideairbnb.com/about/

Collaborators:
Murray Cox, John Morris, Taylor Higgins
Past Collaborators:
Alice Corona, Luca Lamonaca, Michael "Ziggy" Mintz, Anya Sophe Behn



An inline citation example: As discussed on @insideairbnb, there are many...

A parenthetical citation example: There are many ways to research Airbnb [see, for example, @insideairbnb]... 


## 2. Why did they collect the InsideAirbnb data?

::: {.duedate}
@Insideairbnb argued, Airbnb as a privately-owned company, there is currently no mechanism for holding Airbnb accountable for its own actions. 
The public’s ability to see the truth behind Airbnb’s selected data releases is limited. Unfortunately, Airbnb’s so-called transparency initiatives are no substitute for genuine audits or for genuine accountability.
Thus, this organization believes in monitoring and analyzing airbnb's regularly posteddatasets to show the public that majority of Airbnb listings in most cities are entire homes, many of which are rented all year round - disrupting housing and communities.

##Reference 
#Cox, M., and T. Slee. 2016. “How Airbnb’s Data Hid the Facts in New York City.” Inside Airbnb. http://insideairbnb.com/reports/how-airbnbs-data-hid-the-facts-in-new-york-city.pdf
#https://insideairbnb.com/

( 4 points; Answer due Week 7 )

:::

```{python}
#| output: asis
print(f"One of way to embed output in the text looks like this: after cleaning, we were left with {df.shape[0]:,} rows of data.")
```

This way is also supposed to work (`{python} f"{df.shape[0]:,}" `) but I've found it less reliable.

```{python}
ax = df.host_listings_count.plot.hist(bins=50);
ax.set_xlim([0,500]);
```

## 3. How did they collect it?

::: {.duedate}

( 5 points; Answer due Week 8 )
The data sets of @Insideairbnb were assembled by programmatically compiling public information from  Airbnb’s website, but they were implemented and collected independently. Both data sets attempt to locate all the listings within a city, and then visit the page for each listing to collect listing data, including the host ID. The host ID allows an analysis of the number of listings posted by a single host. 
For estimating how often an Airbnb listing is being rented out, and also approximating a listing's income, @Insideairbnb used an occupancy model. They christened the occupancy model as "San Francisco Model", in honor of the public policy and urban planners working for that fair city who created occupancy models to quantify the impact of Airbnb on housing.

- Reference 
    - Cox, M., and T. Slee. 2016. “How Airbnb’s Data Hid the Facts in New York City.” Inside Airbnb. http://insideairbnb.com/reports/how-airbnbs-data-hid-the-facts-in-new-york-city.pdf
    - https://insideairbnb.com/data-assumptions/ 

:::

## 4. How does the method of collection (Q3) impact the completeness and/or accuracy of the InsideAirbnb data? How well does it represent the process it seeks to study, and what wider issues does this raise?

::: {.duedate}

( 11 points; Answer due Week 9 )

:::



## 5. What ethical considerations does the use of the InsideAirbnb data raise? 

::: {.duedate}

( 18 points; Answer due {{< var assess.group-date >}} )

:::

- Privacy and Data Protection: Ensuring that personal data is collected, stored, and used securely, with proper safeguards to prevent unauthorized access or breaches. This includes adhering to laws like GDPR and ensuring data anonymization or encryption when necessary.

- Informed Consent: Data collectors must obtain explicit and informed consent from participants, making sure they understand how their data will be used, especially when dealing with sensitive information.

- Bias and Discrimination: Data analysis can inadvertently introduce or perpetuate bias, which can lead to discriminatory outcomes. For example, certain groups may be overrepresented or underrepresented, causing skewed results or unfair treatment.


## 6. With reference to the InsideAirbnb data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and the types of properties that they list suggest about the nature of Airbnb lettings in London? 

::: {.duedate}

( 15 points; Answer due {{< var assess.group-date >}} )

:::

While exploring @Insideairbnb data, we found that many hosts had multiple listings, and some listings had highly similar neighborhood overviews and descriptions, with locations that were very close to each other. This raises a question of whether this indicates a potential commercial operation model, commonly known as "ghost hotel". The "ghost hotel" refers to an entire apartment building which is functionally a hotel because most or all units are short-term rentals instead of tenant-occupied. However, such properties are unsupervised and do not comply with the same stringent health, safety and planning regulations required by law.

These "ghost hotels" not only cause unfair competition to traditional hotel industry, but their high returns from short-term rentals also attract more hosts to convert their properties to short-term rentals, directly reducing the number of long-term rentals and pushing up rents, making it difficult for local residents to find affordable housing.

How to identify ghost hotels from @Insideairbnb data is key to analysing this phenomenon. Since studies on ghost hotels are limited, we combined observed characteristics and previous studies to determine the method for identification. First, we found the hosts with multiple listings, calculated the geographic distance for all of their listings and used NLP to measure the similarity of neighborhood overviews and descriptions. If the distance is less than 200 meters and the text description similarity is greater than 0.5, it will be considered as potential ghost hotel. We further verified the filtered listings and found that some of the hosts with a large number of listings were not actually individuals, but legally registered companies. These types of listings, while different from traditional hotels, do not belong to the ghost hotel category. By examining host ID frequency distribution, we found that most hosts have 1-4 listings, while those with more than 7 listings are significantly fewer. Therefore, we set 7 as the threshold and considered hosts with more than this number as more likely to be company and excluded them. We finally identified 22,276 potential ghost hotels.

```{python}
stackFigure()
```

Figure 1 shows the proportion of ghost hotels in Airbnb listings in the 33 boroughs of London. It can be seen that the total number of listings in boroughs like Westminster, Tower Hamlets, Hackney are significantly greater than that in others, and the number of ghost hotels in these areas is also noticeably higher.

```{python}
ghProportionPlot()
```

Figure 2 shows the density of ghost hotels in each London borough. It can be seen that the areas of high density are concentrated in central London, where Westminster is the highest, while the outer boroughs have significantly lower densities.



## 7. Drawing on your previous answers, and supporting your response with evidence (*e.g.* figures, maps, EDA/ESDA, and simple statistical analysis/models drawing on experience from, e.g., CASA0007), how *could* the InsideAirbnb data set be used to inform the regulation of Short-Term Lets (STL) in London? 

::: {.duedate}

( 45 points; Answer due {{< var assess.group-date >}} )

:::




## Sustainable Authorship Tools

Using the Terminal in Docker, you compile the Quarto report using `quarto render <group_submission_file>.qmd`.

Your QMD file should automatically download your BibTeX and CLS files and any other required files. If this is done right after library loading then the entire report should output successfully.

Written in Markdown and generated from [Quarto](https://quarto.org/). Fonts used: [Spectral](https://fonts.google.com/specimen/Spectral) (mainfont), [Roboto](https://fonts.google.com/specimen/Roboto) (<span style="font-family:Sans-Serif;">sansfont</span>) and [JetBrains Mono](https://fonts.google.com/specimen/JetBrains%20Mono) (`monofont`). 

## References

